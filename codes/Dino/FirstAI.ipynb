{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytesseract\n",
    "# %pip install swig\n",
    "# %pip install python-opencv\n",
    "# %pip install gymnasium[all] \n",
    "# %pip install mss pydirectinput\n",
    "# Install stable-baselines3 for gymnasium\n",
    "# %pip install git+https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "# All scripts are available in the venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Image capture and display\n",
    "from mss import mss\n",
    "import cv2\n",
    "\n",
    "#Stable baselines imports\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Environment and I/O tools\n",
    "import pydirectinput\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe' #Change installation location\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "#If you are going for keras-rl2 use below\n",
    "# from keras import Sequential\n",
    "# from keras.layers import Dense, Flatten\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255, shape=(1, 83, 120), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        self.cap = mss()\n",
    "        self.agent_location = {'top': 300,\n",
    "                               'left': 0, 'width': 600, 'height': 500}\n",
    "        self.done_location = {'top': 405,\n",
    "                              'left': 630, 'width': 660, 'height': 70}\n",
    "\n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0: 'space',  # Jump\n",
    "            1: 'down',  # Duck\n",
    "            2: 'no_op'  # Run\n",
    "        }\n",
    "        if action != 2:\n",
    "            pydirectinput.press(action_map[action])  # type: ignore\n",
    "        over, over_cap = self.game_over()\n",
    "        next_obs = self.get_observation()\n",
    "        reward = 10\n",
    "        truncation = 0  # Limit is 99999, then score changes to zero\n",
    "        info = {}\n",
    "        return next_obs, reward, over, truncation, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        pydirectinput.click(x=150, y=150)\n",
    "        pydirectinput.press('space')\n",
    "        return self.get_observation(), {}\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self):\n",
    "        # Screen capture\n",
    "        raw = np.array(self.cap.grab(self.agent_location))[:, :, :3]\n",
    "        # convert to greyscale(reduces size)\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        # resize to match observation space\n",
    "        resize = cv2.resize(gray, (120, 83))\n",
    "        # reshape for pytorch\n",
    "        channel = np.reshape(resize, (1, 83, 120))\n",
    "        return channel\n",
    "\n",
    "    def game_over(self):\n",
    "        over_cap = np.array(self.cap.grab(self.done_location))[:, :, :3]\n",
    "        over_ind = ['GAME', 'GAHE']\n",
    "        over = False\n",
    "        res = pytesseract.image_to_string(over_cap)[:4]\n",
    "        if res in over_ind:\n",
    "            over = True\n",
    "        return over, over_cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating environment\n",
    "env = GameEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check action space\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate observation for the bot\n",
    "obs = env.get_observation()\n",
    "plt.imshow(cv2.cvtColor(obs[0], cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_states = env.observation_space.shape\n",
    "# # tot_actions = 3 #Figure out later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom logger for action callback\n",
    "\n",
    "class TrainLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(\n",
    "                self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'C:/Users/nikit/Desktop/Personal Projects/RLmodels/codes/Dino/train'\n",
    "LOG_DIR = 'C:/Users/nikit/Desktop/Personal Projects/RLmodels/codes/Dino/logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1000000  # Total number of steps for training\n",
    "batch_size = 50000  # Number of steps per training iteration (change per RAM in machine)\n",
    "\n",
    "# Create the DQN model\n",
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1,\n",
    "            buffer_size=batch_size, learning_starts=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to load pre-trained models\n",
    "# model.load(\n",
    "#     'C:/Users/nikit/Desktop/Personal Projects/RLmodels/codes/Dino/train/best_model_84000.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works better with high memory machine\n",
    "# model.learn(total_timesteps=total_timesteps, callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For low memory users \n",
    "# Perform incremental learning in multiple iterations\n",
    "current_timestep = 0\n",
    "while current_timestep < total_timesteps:\n",
    "    # Calculate the number of steps for this iteration\n",
    "    remaining_timesteps = total_timesteps - current_timestep\n",
    "    num_steps = min(batch_size, remaining_timesteps)\n",
    "\n",
    "    # Train the model for the current iteration\n",
    "    model.learn(total_timesteps=num_steps, callback=callback)\n",
    "\n",
    "    # Update the current timestep\n",
    "    current_timestep += num_steps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(5):\n",
    "    obs = env.reset()[0]\n",
    "    over = False\n",
    "    tot_reward = 0\n",
    "\n",
    "    while not over:\n",
    "        action = model.predict(obs)\n",
    "        obs, reward, over, info, _ = env.step(env.action_space.sample())\n",
    "        # time.sleep(0.01)\n",
    "        tot_reward += reward\n",
    "    print(f'Episode : {episode}, Reward : {tot_reward}')\n",
    "    time.sleep(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
